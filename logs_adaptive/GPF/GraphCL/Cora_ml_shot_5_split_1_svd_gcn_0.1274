load LLC or attacked shot data with default split
{'scenario': 'poisoning', 'split': 0, 'adaptive_attack_model': 'svd_gcn', 'ptb_rate': 0.1274}
======================
Number of graphs: 1
Number of features: 1433
Number of classes: 7

Data(x=[2485, 1433], edge_index=[2, 13915], y=[2485], train_mask=[2485], val_mask=[2485], test_mask=[2485])
===========================================================================================================
Number of nodes: 2485
Number of edges: 13915
Average node degree: 5.60
Has isolated nodes: False
Has self-loops: True
Is undirected: True
None
tensor([1756,  307,   33, 1404,  188, 2052,  775,  774, 2081, 1717, 2068,  689,
         254, 1280,  225,  870, 2131, 1445, 1204, 1459, 1394, 1120,  728, 1165,
        1540, 2302, 1798,  937, 1929, 1764, 1142, 2143, 1830,  532,  803])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,
        4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6])
len train nodes: 35
len val   nodes: 245
len test  nodes: 2205
Successfully loaded pre-trained weights!
prepare induce graph data is finished!
run GPF
Epoch 001 |  Time(s) 0.8210 | GPF Loss 1.9294  
Epoch 002 |  Time(s) 0.0407 | GPF Loss 1.9692  
Epoch 003 |  Time(s) 0.0416 | GPF Loss 1.8668  
Epoch 004 |  Time(s) 0.0359 | GPF Loss 1.9144  
Epoch 005 |  Time(s) 0.0363 | GPF Loss 1.8029  
Epoch 006 |  Time(s) 0.0366 | GPF Loss 1.8975  
Epoch 007 |  Time(s) 0.0342 | GPF Loss 1.8540  
Epoch 008 |  Time(s) 0.0341 | GPF Loss 1.8503  
Epoch 009 |  Time(s) 0.0340 | GPF Loss 1.7868  
Epoch 010 |  Time(s) 0.0339 | GPF Loss 1.8196  
Epoch 011 |  Time(s) 0.0346 | GPF Loss 1.8174  
Epoch 012 |  Time(s) 0.0337 | GPF Loss 1.7784  
Epoch 013 |  Time(s) 0.0338 | GPF Loss 1.7821  
Epoch 014 |  Time(s) 0.0336 | GPF Loss 1.7801  
Epoch 015 |  Time(s) 0.0331 | GPF Loss 1.7590  
Epoch 016 |  Time(s) 0.0337 | GPF Loss 1.7665  
Epoch 017 |  Time(s) 0.0331 | GPF Loss 1.7600  
Epoch 018 |  Time(s) 0.0344 | GPF Loss 1.7714  
Epoch 019 |  Time(s) 0.0332 | GPF Loss 1.7489  
Epoch 020 |  Time(s) 0.0336 | GPF Loss 1.7638  
Epoch 021 |  Time(s) 0.0332 | GPF Loss 1.7582  
Epoch 022 |  Time(s) 0.0331 | GPF Loss 1.7398  
Epoch 023 |  Time(s) 0.0330 | GPF Loss 1.7874  
Epoch 024 |  Time(s) 0.0335 | GPF Loss 1.7430  
Epoch 025 |  Time(s) 0.0332 | GPF Loss 1.7751  
Epoch 026 |  Time(s) 0.0337 | GPF Loss 1.7540  
Epoch 027 |  Time(s) 0.0340 | GPF Loss 1.7468  
Epoch 028 |  Time(s) 0.0340 | GPF Loss 1.7546  
Epoch 029 |  Time(s) 0.0331 | GPF Loss 1.7580  
Epoch 030 |  Time(s) 0.0332 | GPF Loss 1.7530  
Epoch 031 |  Time(s) 0.0333 | GPF Loss 1.7241  
Epoch 032 |  Time(s) 0.0344 | GPF Loss 1.7459  
Epoch 033 |  Time(s) 0.0330 | GPF Loss 1.7269  
Epoch 034 |  Time(s) 0.0327 | GPF Loss 1.7744  
Epoch 035 |  Time(s) 0.0328 | GPF Loss 1.7173  
Epoch 036 |  Time(s) 0.0334 | GPF Loss 1.7410  
Epoch 037 |  Time(s) 0.0336 | GPF Loss 1.7452  
Epoch 038 |  Time(s) 0.0329 | GPF Loss 1.7346  
Epoch 039 |  Time(s) 0.0355 | GPF Loss 1.7271  
Epoch 040 |  Time(s) 0.0333 | GPF Loss 1.7112  
Epoch 041 |  Time(s) 0.0331 | GPF Loss 1.7383  
Epoch 042 |  Time(s) 0.0352 | GPF Loss 1.7256  
Epoch 043 |  Time(s) 0.0381 | GPF Loss 1.7171  
Epoch 044 |  Time(s) 0.1577 | GPF Loss 1.6978  
Epoch 045 |  Time(s) 0.0393 | GPF Loss 1.6896  
Epoch 046 |  Time(s) 0.0342 | GPF Loss 1.7044  
Epoch 047 |  Time(s) 0.2158 | GPF Loss 1.6738  
Epoch 048 |  Time(s) 0.0373 | GPF Loss 1.6876  
Epoch 049 |  Time(s) 0.1123 | GPF Loss 1.6888  
Epoch 050 |  Time(s) 0.0348 | GPF Loss 1.6760  
Epoch 051 |  Time(s) 0.0395 | GPF Loss 1.6560  
Epoch 052 |  Time(s) 0.0335 | GPF Loss 1.6729  
Epoch 053 |  Time(s) 0.0362 | GPF Loss 1.6644  
Epoch 054 |  Time(s) 0.0422 | GPF Loss 1.6715  
Epoch 055 |  Time(s) 0.0363 | GPF Loss 1.6702  
Epoch 056 |  Time(s) 0.0494 | GPF Loss 1.6911  
Epoch 057 |  Time(s) 0.0373 | GPF Loss 1.7268  
Epoch 058 |  Time(s) 0.0394 | GPF Loss 1.6613  
Epoch 059 |  Time(s) 0.0336 | GPF Loss 1.7454  
Epoch 060 |  Time(s) 0.0361 | GPF Loss 1.8001  
Epoch 061 |  Time(s) 0.0354 | GPF Loss 1.7689  
Epoch 062 |  Time(s) 0.0353 | GPF Loss 1.7129  
Epoch 063 |  Time(s) 0.0352 | GPF Loss 1.7937  
Epoch 064 |  Time(s) 0.0336 | GPF Loss 1.7800  
Epoch 065 |  Time(s) 0.0476 | GPF Loss 1.7887  
Epoch 066 |  Time(s) 0.0360 | GPF Loss 1.8009  
Epoch 067 |  Time(s) 0.0388 | GPF Loss 1.8407  
Epoch 068 |  Time(s) 0.0344 | GPF Loss 1.8151  
Epoch 069 |  Time(s) 0.0411 | GPF Loss 1.8016  
Epoch 070 |  Time(s) 0.0667 | GPF Loss 1.8349  
----------------------------------------------------------------------------------------------------
Early stopping at 71 eopch!
Batch 0/23 Acc: 0.3800 | Macro-F1: 0.1968
Batch 1/23 Acc: 0.2200 | Macro-F1: 0.1353
Batch 2/23 Acc: 0.1900 | Macro-F1: 0.1225
Batch 3/23 Acc: 0.1900 | Macro-F1: 0.0980
Batch 4/23 Acc: 0.4400 | Macro-F1: 0.2077
Batch 5/23 Acc: 0.3200 | Macro-F1: 0.1832
Batch 6/23 Acc: 0.3100 | Macro-F1: 0.1601
Batch 7/23 Acc: 0.3100 | Macro-F1: 0.1840
Batch 8/23 Acc: 0.3000 | Macro-F1: 0.1833
Batch 9/23 Acc: 0.2900 | Macro-F1: 0.1711
Batch 10/23 Acc: 0.2700 | Macro-F1: 0.1749
Batch 11/23 Acc: 0.3000 | Macro-F1: 0.1584
Batch 12/23 Acc: 0.3000 | Macro-F1: 0.1687
Batch 13/23 Acc: 0.2900 | Macro-F1: 0.1571
Batch 14/23 Acc: 0.2200 | Macro-F1: 0.1378
Batch 15/23 Acc: 0.4300 | Macro-F1: 0.2349
Batch 16/23 Acc: 0.4800 | Macro-F1: 0.2266
Batch 17/23 Acc: 0.4100 | Macro-F1: 0.1585
Batch 18/23 Acc: 0.4200 | Macro-F1: 0.2126
Batch 19/23 Acc: 0.2700 | Macro-F1: 0.1666
Batch 20/23 Acc: 0.1800 | Macro-F1: 0.1094
Batch 21/23 Acc: 0.2800 | Macro-F1: 0.1459
Batch 22/23 Acc: 0.2000 | Macro-F1: 0.1111
Final True Accuracy: 0.3088 | Macro F1 Score: 0.1798
best_loss [1.8260351419448853]
{1: {1: 0.3088435232639313}}
{1: {1: 0.3088435232639313}}
########################################################################################
seed: 1 | split 1 : 0.3088435232639313
# Seed 1 Muti Split Final Acc: 0.3088±0.0000
########################################################################################
There's only one result, it's recommended to try several seeds.
split: 1 | seed 1 : 0.3088435232639313
# Split 1 Muti Seed Acc without min value: 0.3088±0.0000
########################################################################################
